{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nSince the authors are very new to Machine learning, Artifical Intelligence and less than average 2 years of experience in Python, this might be helpful for such newbies like them. Compared to other good guides, the notebook is disorganised and confusing. We think this is also a difficulty that people at a similar level to the authors may experience, so we left the content as unrefined as possible.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Data analysis\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random as rnd\n\n# Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Common Model Helpers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# ML Algorithms\nfrom sklearn import ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis\n\n# Performance mesurement\nfrom time import perf_counter\n\n# Load data for training\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")       # Raw data for training\nvalidation_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")   # Test data for validation\n\nTF_RANDOM_SEED = 1234\n\ndef format_percentile(value, total):\n    return float(f\"{(value / total) * 100:.2f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:31.927418Z","iopub.execute_input":"2025-01-24T08:07:31.927904Z","iopub.status.idle":"2025-01-24T08:07:31.952315Z","shell.execute_reply.started":"2025-01-24T08:07:31.927870Z","shell.execute_reply":"2025-01-24T08:07:31.950922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preprocessing\n\nBefore training the model, we preprocess the data to increase efficiency and obtain more accurate results. Especially, checking any inaccurate or missing data is very important. In this example, there are 12 columns, and it is not pleasant to watch our model fails to predict because of some columns which seems to non-vital for decision are missing. Therefore, we analyze the data first and find as many correlations as possible.","metadata":{}},{"cell_type":"markdown","source":"## Type definition at glimpse\n\nData per columns could be analysed as:\n\n| Column name   | Meaning                         | Data series                      | Semantics                    | Categorised as               |\n|---------------|---------------------------------|----------------------------------|------------------------------|------------------------------|\n| `PassengerId` | Passenger identifier            | \\[`0`, `1`, ... `Infinity`]      | n/a                          | Non-negative Integer         |\n| `Survived`    | Survival                        | \\[`0`, `1`]                      | `0`: False, `1`: True        | Binary                       |\n| `PClass`      | Passenger class                 | \\[`1`, `2`, `3`]                 | `1`: 1st, `2`: 2nd, `3`: 3rd | Enum                         |\n| `Name`        | Passenger name                  | n/a                              | n/a                          | String                       |\n| `Sex`         | Passenger gender                | \\[`male`, `female` ]             | ...                          | Enum                         |\n| `Age`         | Passenger age                   | \\[`1`, `2`, ... `Infinity`]      | n/a                          | Positive Integer             |\n| `SibSp`       | No. of Siblings/Spouses Aboard  | \\[`1`, `2`, ... `Infinity`]      | n/a                          | Positive Integer             |\n| `Parch`       | No. of Parents/Children Aboard  | \\[`1`, `2`, ... `Infinity`]      | n/a                          | Positive Integer             |\n| `Ticket`      | Ticket Number                   | n/a                              | n/a                          | String                       |\n| `Fare`        | Passenger fare                  | \\[`0.0`, `0.1`, ... `Infinity` ] | n/a                          | Non-negative rational Number |\n| `Cabin`       | Passenger cabin                 | n/a                              | n/a                          | String                       |\n| `Embarked`    | Port of Embarkation             | \\[`C`, `Q`, `S` ]                | Capital letter of city name  | Enum                         |","metadata":{}},{"cell_type":"markdown","source":"## Data exploration\n\nHence there are missing values in training data at glimpse and they makes our model difficult to train, we have to find them first.","metadata":{}},{"cell_type":"code","source":"def find_missing_columns(data_frame, columns):\n    # Find records with missing data(all records that contains \"NaN\" in any column)\n    records_with_nan = data_frame[data_frame.isnull().any(axis=1)]\n    total_count = len(data_frame)\n    \n    # Iterate over each column to find and display missing data\n    for column in columns:\n        missing_records = data_frame[data_frame[column].isnull()]\n        missing_count = len(missing_records)\n        \n        if missing_count > 0:\n            missing_rate = format_percentile(missing_count, total_count)\n            print(f\"Records that \\\"{column}\\\" column is missing: {missing_count} of total {total_count} ({missing_rate}%)\")\n            # passenger_ids = \", \".join(map(str, missing_records[\"PassengerId\"].to_list()))\n            # print(f\"PassengerIds: {passenger_ids}\")\n\nprint(\"In training data:\")\nfind_missing_columns(train_data, [\"Ticket\", \"Name\", \"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"])\n\nprint()\n\nprint(\"In validation data:\")\nfind_missing_columns(train_data, [\"Ticket\", \"Name\", \"PassengerId\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:31.953869Z","iopub.execute_input":"2025-01-24T08:07:31.954232Z","iopub.status.idle":"2025-01-24T08:07:31.985435Z","shell.execute_reply.started":"2025-01-24T08:07:31.954193Z","shell.execute_reply":"2025-01-24T08:07:31.983430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We didn't expect that there are incomplete in test data as well. This means actual input may contain missing data in any columns, so we must figure a completion strategy out before training.","metadata":{}},{"cell_type":"markdown","source":"## Data completion\n\nBased on the results,  we're going to find interpolatable or removable values to improve data clarity and expect model effectiveness.","metadata":{}},{"cell_type":"markdown","source":"### Optimisation\n\nLet's look at some data that seems safe to remove.\n\n- `PassengerId` is a value for simple information distinction, so it does not affect the survival of passengers. However, it is important for identifying upon analysation stage, so we're going to remove it at the just beginning of the training stage.\n- It seems that no one boarded without a `Ticket`. Also, passenger information such as name, cabin class, fare, and family status that can be inferred from the `Ticket` is well entered in other columns, so it seems safe to ignore it.\n\nWhat about handling ambiguous data?\n\n- `Cabin` is information that indicates the location of the cabin and considering by intuition, it is a factor that affects the survival rate, therefore it would be very important information. However, there are too many records that lack `Cabin`. Moreover, while `Age`, `Embarked`, and `Fare` can be expressed as continuous values, `Cabin` is difficult to interpolate properly without additionally obtaining the Titanic's blueprint. Therefore, we boldly remove `Cabin` in training data. It will be ignored upon inference.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(columns=[\"Ticket\", \"Cabin\"])  # PassengerId is subject to removal just before training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:31.988238Z","iopub.execute_input":"2025-01-24T08:07:31.988686Z","iopub.status.idle":"2025-01-24T08:07:31.996748Z","shell.execute_reply.started":"2025-01-24T08:07:31.988639Z","shell.execute_reply":"2025-01-24T08:07:31.994630Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpolation\n\n","metadata":{}},{"cell_type":"markdown","source":"`Age` is a natural number, `Embarked` is an enum, and `Fare` is a rational number. All these three information can be expressed as continuous numbers. Therefore, we can apply interpolation methods on them using statistical tools such as the mean, standard deviation, and variance. However, we yet don't have a good determination of interpolation methods. So let's find out more correlations and then choose proper strategy.","metadata":{}},{"cell_type":"code","source":"plt.subplots_adjust(wspace=20, hspace=100)\nplt.figure(figsize=[24,10])\n\n# Draws histogram of clumn that contains series of discrete values such as integers, binaries, etc.\n# Complex data like nominal values(enums in this case) cannot be plotted.\ndef draw_survival_histogram(subplot_coordinates, data, column, x_axis_ticks=None, x_axis_labels=None):\n    plt.subplot(*subplot_coordinates)\n    plt.hist(x = [data[data[\"Survived\"]==1][column], data[data[\"Survived\"]==0][column]], \n             stacked=True,\n             color = ['g','r'],\n             label = [\"Survived\",\"Dead\"])\n    plt.title(f\"{column} Histogram by Survival\")\n    plt.xlabel(column)\n    plt.ylabel(\"# of Passengers\")\n    if x_axis_ticks is not None and x_axis_labels is not None:\n        plt.xticks(x_axis_ticks, x_axis_labels)\n    elif x_axis_ticks is not None:\n        plt.xticks(x_axis_ticks)\n    plt.legend()\n    \ndraw_survival_histogram([2, 3, 1], train_data, \"Sex\",)\ndraw_survival_histogram([2, 3, 2], train_data, \"Age\",)\ndraw_survival_histogram([2, 3, 3], train_data, \"Pclass\", x_axis_ticks = [1, 2, 3])\ndraw_survival_histogram([2, 3, 4], train_data, \"Fare\")\n\n# Nominal values that are more than 2 variations cannot be plotted, so we have to reformat it first.\ntrain_data[\"Embarked_Code\"] = train_data[\"Embarked\"].map(lambda it: {\"C\": 1, \"Q\": 2, \"S\": 3}.get(it, 4))\ndraw_survival_histogram([2, 3, 5], train_data, \"Embarked_Code\", x_axis_ticks = [1, 2, 3, 4], x_axis_labels = [\"C\", \"Q\", \"S\", \"Unknown\"])\ntrain_data = train_data.drop(columns=[\"Embarked_Code\"])  # Only required for plotting, not necessary afterwards","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:31.998752Z","iopub.execute_input":"2025-01-24T08:07:31.999108Z","iopub.status.idle":"2025-01-24T08:07:33.460617Z","shell.execute_reply.started":"2025-01-24T08:07:31.999077Z","shell.execute_reply":"2025-01-24T08:07:33.459317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When looking at the histogram above, we can conclude that a record with these characteristics:\n\n- Female(`Sex` = female)\n- High Fare(`Fare` > 50.0)\n- Higher PClass(`PClass` = 1)\n- Embarked = C\n- Age < 10.\n\nshows a tendency of high probability of survival. We can say that \"A female passenger embarked from C, younger than 10 years, with high social status quo and able to pay more money, is more likely to survive\". \n\nFor missing values such as `Age`, `Fare` that can be expressed numerically, it seems better to use median rather than arithmetic mean. We can check the arithmetic mean and median of each data in the following graph, and find that the median has less deviation. In statistics, it is common to select the median to obtain robust results, so selecting the median seems to be a good strategy.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[24,4])\n\ndef draw_mean_and_median_by_survived(subplot_coordinates, data, column):\n    summary_table = train_data.groupby(\"Survived\")[column].agg([\"mean\", \"median\"])\n\n    # Rename the columns to match the desired format\n    summary_table.columns = [\"mean\", \"median\"]\n\n    # Plot as a stacked bar chart\n    labels = [\"Not Survived (0)\", \"Survived (1)\"]\n    mean_values = summary_table[\"mean\"]\n    median_values = summary_table[\"median\"]\n\n    # Create the bar positions\n    x = range(len(labels))  # Positions for each group\n    bar_width = 0.35  # Width of each bar\n\n    plt.subplot(*subplot_coordinates)\n    plt.bar([pos - bar_width / 2 for pos in x], mean_values, width=bar_width, label=\"Mean\", color=\"skyblue\")\n    plt.bar([pos + bar_width / 2 for pos in x], median_values, width=bar_width, label=\"Median\", color=\"orange\")\n    \n    # Add labels and title\n    plt.xticks(x, labels)  # Set x-axis labels\n    plt.xlabel(\"Survival Status\")\n    plt.ylabel(column)\n    plt.title(f\"{column} mean and median\")\n    plt.legend()\n\ndraw_mean_and_median_by_survived([1, 2, 1], train_data, \"Age\")\ndraw_mean_and_median_by_survived([1, 2, 2], train_data, \"Fare\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:33.461841Z","iopub.execute_input":"2025-01-24T08:07:33.462299Z","iopub.status.idle":"2025-01-24T08:07:33.926833Z","shell.execute_reply.started":"2025-01-24T08:07:33.462254Z","shell.execute_reply":"2025-01-24T08:07:33.925497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Thus we fill missing `Age` and `Fare` by medians as follows.","metadata":{}},{"cell_type":"code","source":"def fill_null_by_median(data, column):\n    data[column] = data[column].fillna(data[column].median())\n\nfill_null_by_median(train_data, \"Age\")\nfill_null_by_median(train_data, \"Fare\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:33.928013Z","iopub.execute_input":"2025-01-24T08:07:33.928418Z","iopub.status.idle":"2025-01-24T08:07:33.934511Z","shell.execute_reply.started":"2025-01-24T08:07:33.928386Z","shell.execute_reply":"2025-01-24T08:07:33.933414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"However, `Embarked` is expressed as nominal value and taking median of it seems useless. Threfore, we make a heuristic approach to find similar value from other records, that has similar `Survival`, `Sex`, `Age`, `Pclass`, `Fare` values.","metadata":{}},{"cell_type":"code","source":"def fill_missing_embarked(row, train_data):\n    # If Embarked is already present, return it\n    if pd.notna(row['Embarked']):\n        return row['Embarked']\n    \n    # Step 1: Filter by same Survival value\n    filtered_data = train_data[\n        (train_data['Survived'] == row['Survived']) & \n        (train_data['Embarked'].notna())  # Only consider records with Embarked values\n    ]\n    \n    # Step 2: Filter by same Sex value\n    filtered_data = filtered_data[filtered_data['Sex'] == row['Sex']]\n    \n    # Step 3: Filter by same Pclass value\n    filtered_data = filtered_data[filtered_data['Pclass'] == row['Pclass']]\n    \n    # If no matches found after filtering, return most common Embarked value\n    if filtered_data.empty:\n        return train_data['Embarked'].mode()[0]\n    \n    # Step 4: Sort by Age difference if Age is available\n    if pd.notna(row['Age']):\n        filtered_data['AgeDiff'] = abs(filtered_data['Age'] - row['Age'])\n        filtered_data = filtered_data.sort_values('AgeDiff')\n    \n    # Step 5: Return the Embarked value of the first matching record\n    return filtered_data.iloc[0]['Embarked']\n\ntrain_data['Embarked'] = train_data.apply(lambda row: fill_missing_embarked(row, train_data), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:33.935547Z","iopub.execute_input":"2025-01-24T08:07:33.935817Z","iopub.status.idle":"2025-01-24T08:07:33.961820Z","shell.execute_reply.started":"2025-01-24T08:07:33.935793Z","shell.execute_reply":"2025-01-24T08:07:33.960572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And confirm that there are no records with any `NaN`s.","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:33.964551Z","iopub.execute_input":"2025-01-24T08:07:33.964904Z","iopub.status.idle":"2025-01-24T08:07:33.977002Z","shell.execute_reply.started":"2025-01-24T08:07:33.964874Z","shell.execute_reply":"2025-01-24T08:07:33.975662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data conversion\n\nSince most machine learning algorithm can effectively handle numerical values, and to make our model not overfit to test data, it is better to do some conversions.","metadata":{}},{"cell_type":"markdown","source":"### Feature aggregation(Feature composition)\n\nIn some cases creating a new information by combining multiple, similar data might capture complex patterns that individual features and miss decrease model complexity. In this problem, we can derive a new feature, named `FamilySize` from `SibSp` and `Parch`. After aggregation, we can remove `SibSp` and `Parch` for simplicity.","metadata":{}},{"cell_type":"code","source":"def fill_family_size(data):\n    # +1 for passenger him/herself\n    data[\"FamilySize\"] = data.apply(lambda row: row ['SibSp'] + row['Parch'] + 1, axis=1)\n    return data.drop(['Parch', 'SibSp'], axis=1)\n\ntrain_data = fill_family_size(train_data)\n\ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:33.978523Z","iopub.execute_input":"2025-01-24T08:07:33.979006Z","iopub.status.idle":"2025-01-24T08:07:34.002984Z","shell.execute_reply.started":"2025-01-24T08:07:33.978967Z","shell.execute_reply":"2025-01-24T08:07:34.001754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### One-hot encoding (Dummy variable method)\n\nBefore training, we have to convert nominal values(`enum`s) to dummy variables. Moreover, we must not convert those nominal values directly to numerical values - those must be represented in different way. This is called **dummy variable** or **one-hot encoding** method. Without the method, there would be huge disadvantages such as:\n\n- Most ML algorithms will simply fail to run because they require numerical values\n- Reduced Model performance because:\n  * Decision tree algorithm may converge incorrectly on numeric-encoded categories.\n  * Neural network may incorrectly weigh the importance of different categories.\n- Model misinterpretation such as:\n  * female(1) is greater than male(0)\n  * With red(1), green(2), blue(3), the difference between blue and red means green.\n\nTherefore we should encode nominal values to dummy variables as following table:\n\n| Original   | One-hot encoded fields                   |\n|------------|------------------------------------------|\n| `Sex`      | `Sex_female`, `Sex_male`                 |\n| `Embarked` | `Embarked_C`, `Embarked_Q`, `Embarked_S` |\n","metadata":{}},{"cell_type":"code","source":"def one_hot_encode(data, columns, prefixes):\n    return pd.get_dummies(data, columns=columns, prefix=prefixes, dtype=\"int64\")\n\ntrain_data = one_hot_encode(train_data, columns=[\"Sex\", \"Embarked\"], prefixes=[\"Sex\", \"Embarked\"])\n\ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.004452Z","iopub.execute_input":"2025-01-24T08:07:34.004938Z","iopub.status.idle":"2025-01-24T08:07:34.023355Z","shell.execute_reply.started":"2025-01-24T08:07:34.004892Z","shell.execute_reply":"2025-01-24T08:07:34.021800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Binning (Bucketing)\n\nWe also consider binning our data. **Binning**(or **Bucketing**) is a technique that transforms continuous numerical values into discrete categories or groups. For example, `Fare` could be binned as:\n\n- 0-100 Low fare\n- 100-200 Medium fare\n- 200-300 High fare\n- 300+ Very high fare\n\n#### Why?\n\n1. Simplifies non-linear relationships to help capturing broader patterns that might be missed in raw data. For example, a model applied binning may treat fare 8.5 and 8.6 as similar value since they might be contained in same bin. Without binning, on the contrary, such model treats the value completely differently and there could be a model performance degradation.\n2. Reduces model complexity since continuous range of numbers are reduced to finite set of categories.\n3. Reduces noise or outliers. Extreme values can skew statistical analysis; however binning captures those values within categories, thus it prevents individual extreme values from dominating model interpretation.\n4. Could highlight meaningful thresholds or group data in ways that help the model learn better patterns.\n\n#### Advantages\n\n1. Model simplicity.\n    - Simplifies the feature space by reducing continuous data into categories.\n    - Helps less flexible algorithms, like linear regression, perform better by reducing noise.\n\n2. Improved robustness & Noise reduction.\n    - Mitigates the impact of outliers, as extreme values are grouped into a bin rather than being treated individually.\n\n3. Pattern discovery & Bettter interpretability.\n    - Humans can more easily understand \"age groups\" or \"fare ranges\" compared to raw continuous values.\n\n#### Disadvantages\n\n1. Loss of Granular Information.\n    - Binning reduces the granularity of the data, potentially discarding important information about the relationship between the feature and the target. For example, \"Fare = $10.01\" and \"Fare = $19.99\" might be placed in the same bin, even though they could have different effects on target(`Survival` in this case).\n\n2. Biased model.\n    - The choice of the number of bins and their ranges can introduce bias. For instance, is splitting \"Age\" into 5 bins the best choice? What about 10?\n\n3. Loss of Distribution Detail.\n    - Continuous variables often have rich distributions that may carry meaningful patterns. Binning simplifies these patterns, potentially leading to suboptimal results.\n\n#### In action\n\n- Use Binning if:\n    * Using algorithms that perform better on categorical or ordinal data\n    * To reduce the impact of outliers\n    * To get better data interpretability\n\n- Avoid binning if:\n    * Extremely precise measurements are required\n    * In domains requiring fine-grained analysis\n    * Data distribution is already uniform\n    * Using algorithms like neural networks or gradient boosting that handle continuous features naturally.\n\nOur data does not meet any avoidances as above, and by intuition this problem could be solved by decision tree algorithm, we now progress on binning. According to the histogram we derived at the data clarification stage, `Fare` and `Age` has definite features as:\n\n| Column | Features                                                                               |\n|--------|----------------------------------------------------------------------------------------|\n| `Fare` | <ul><li>Most passengers paid less than 50 and they death rate is centred in that range.</li> <li>Fare data in certain range(150-200), (300-480) seems does not exist. </li></ul> |\n| `Age`  | Divided in 10-year units, we can observe siginificant fluctuation in 20s, 30s and 40s. |\n\nAnd there are two types of binning which are:\n\n- `qcut()`: Equal-frequency bins (Quantile binning - each bin has similar number of samples)\n- `cut()`: Equal-width bins (Fixed interval binning - each bin covers same range of values)\n\nAccording to the trends and data distribution, we can apply `qcut` on `Fare`s and `cut` on `Age`s as following:","metadata":{}},{"cell_type":"code","source":"def bin_ages_and_fares(data): \n    age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf]\n    age_labels = [\"[0, 10)\", \"[10, 19)\", \"[20, 29)\", \"[30, 39)\", \"[40, 49)\", \"[50, 59)\", \"[60, 69)\", \"[70, 79)\", \"80+\"]\n    age_bins = pd.cut(data[\"Age\"].astype(int), bins=age_bins, labels=age_labels, right=False)\n    \n    # Fares could be roughly grouped in 5, as we can see in the histogram above.\n    fare_bins = pd.qcut(data[\"Fare\"], 5)\n    \n    # Encode bins to ordinal value since they could be compared\n    label = LabelEncoder()\n    data[\"AgeBins_Code\"] = label.fit_transform(age_bins)\n    data[\"FareBins_Code\"] = label.fit_transform(fare_bins)\n    data = data.drop([\"Age\", \"Fare\"], axis=1)\n\n    return data\n\ntrain_data = bin_ages_and_fares(train_data)\n\ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.024622Z","iopub.execute_input":"2025-01-24T08:07:34.024993Z","iopub.status.idle":"2025-01-24T08:07:34.049410Z","shell.execute_reply.started":"2025-01-24T08:07:34.024927Z","shell.execute_reply":"2025-01-24T08:07:34.048244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"and drop `Name` for model simplicity, since `Name` does not affect survival - EDIT: this judgement might inaccurate because in our data `Name` contains social title, which could be a huge hint for age(`Age`) and social status(`Pclass`) -. After this step, our `train_data` consisted with only numerical values, that is ready for ML algorithms that handles numerical values. Also, `PassengerId` is removed as well as we previously notated.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['PassengerId', 'Name'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.050487Z","iopub.execute_input":"2025-01-24T08:07:34.050801Z","iopub.status.idle":"2025-01-24T08:07:34.056473Z","shell.execute_reply.started":"2025-01-24T08:07:34.050772Z","shell.execute_reply":"2025-01-24T08:07:34.055216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.describe(include = \"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.057348Z","iopub.execute_input":"2025-01-24T08:07:34.057644Z","iopub.status.idle":"2025-01-24T08:07:34.093163Z","shell.execute_reply.started":"2025-01-24T08:07:34.057617Z","shell.execute_reply":"2025-01-24T08:07:34.091983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.094270Z","iopub.execute_input":"2025-01-24T08:07:34.094662Z","iopub.status.idle":"2025-01-24T08:07:34.107769Z","shell.execute_reply.started":"2025-01-24T08:07:34.094621Z","shell.execute_reply":"2025-01-24T08:07:34.106682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split training and testing data\n\nIf we use our `train_data` for learning as it is, the model could show 100% accuracy for `train_data`. In other words, it can act as if it is cheating for `train_data`. This is called the model overfitting problem. In this challenge, the total number of data given for learning is 891, so even if it is divided at a certain ratio using `sklearn`'s `train_test_split`, it will not be a big problem because there is still enough data for learning. \n\nTo evaluate that any model is overfitted is, accumulate the loss values of `train_data` and `validation_data` for each learning epoch, and then determine the point when the difference between the two losses begins to increase. The larger the loss deviation, model is overfit.","metadata":{}},{"cell_type":"code","source":"target = train_data[\"Survived\"]\ntrain_data = train_data.drop([\"Survived\"], axis=1)\nx_train, x_val, y_train, y_val = train_test_split(train_data, target, test_size = 0.25, random_state = TF_RANDOM_SEED)\n\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"x_val   shape: {x_val.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val   shape: {y_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.108839Z","iopub.execute_input":"2025-01-24T08:07:34.109186Z","iopub.status.idle":"2025-01-24T08:07:34.119602Z","shell.execute_reply.started":"2025-01-24T08:07:34.109158Z","shell.execute_reply":"2025-01-24T08:07:34.118633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training\n\nOn our long data preprocessing journey, now we're finally ready for model training. Since we're completely new to AI/ML field and don't have experiences for choosing which ML algorithm is the most suitable for this kind of problem. So, list every ML algorithm up and setup a small tool to evaluate which algorithm is the best for this challenge.","metadata":{}},{"cell_type":"code","source":"ml_algorithms = [\n    # Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    # Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    # Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    # Generalised Linear Models\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n\n    # Naive Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    # Support Vector Machine\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    # Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n]\n\n##\n# Performs training\n##\ntable = pd.DataFrame(columns = [\"Name\", \"Parameters\", \"Time Elapsed\", \"Accuracy\"])\n\nrow_index = 0\nfor alg in ml_algorithms:\n    # Reflection\n    alg_name = alg.__class__.__name__\n    table.loc[row_index, \"Name\"] = alg_name\n    table.loc[row_index, \"Parameters\"] = str(alg.get_params())\n\n    start_time = perf_counter()\n    alg.fit(x_train, y_train)\n    end_time = perf_counter()\n    table.loc[row_index, \"Time Elapsed\"] = end_time - start_time\n    y_pred = alg.predict(x_val)\n    table.loc[row_index, \"Accuracy\"] = round(accuracy_score(y_pred, y_val) * 100, 2)\n    row_index += 1\n    \n# Print table\nsorted_table = table.sort_values(by=[\"Accuracy\", \"Time Elapsed\"], ascending=[False, True])\nsorted_table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:34.120588Z","iopub.execute_input":"2025-01-24T08:07:34.120964Z","iopub.status.idle":"2025-01-24T08:07:35.453235Z","shell.execute_reply.started":"2025-01-24T08:07:34.120927Z","shell.execute_reply":"2025-01-24T08:07:35.451719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is an annoying warning, but we don't have enough knowledge to solve it right now and ignore it.\n\nAccording to the evaluation results above, the `RandomForestClassifier` algorithm gives the best results and takes less time than `LogisticRegressionCV`. Therefore, we will submit the final result using `RandomForestClassifier`.","metadata":{}},{"cell_type":"markdown","source":"# Submit prediction results\n\nNow, transform `validation_data` to have same columns and types of `train_data` before training.","metadata":{}},{"cell_type":"code","source":"validation_data_for_predict = validation_data.drop(columns=[\"Ticket\", \"Cabin\"])\nfill_null_by_median(validation_data_for_predict, \"Age\")\nfill_null_by_median(validation_data_for_predict, \"Fare\")\nvalidation_data_for_predict[\"Embarked\"] = validation_data_for_predict.apply(lambda row: fill_missing_embarked(row, validation_data_for_predict), axis=1)\nvalidation_data_for_predict = fill_family_size(validation_data_for_predict)\nvalidation_data_for_predict = one_hot_encode(validation_data_for_predict, columns=[\"Sex\", \"Embarked\"], prefixes=[\"Sex\", \"Embarked\"])\nvalidation_data_for_predict = bin_ages_and_fares(validation_data_for_predict)\nvalidation_data_for_predict = validation_data_for_predict.drop(['PassengerId', 'Name'], axis=1)\n\nprint(\"train_data information:\")\ntrain_data.info()\nprint()\nprint(\"validation_data_for_predict information:\")\nvalidation_data_for_predict.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:35.454547Z","iopub.execute_input":"2025-01-24T08:07:35.454921Z","iopub.status.idle":"2025-01-24T08:07:35.501690Z","shell.execute_reply.started":"2025-01-24T08:07:35.454873Z","shell.execute_reply":"2025-01-24T08:07:35.500565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ensemble.RandomForestClassifier()\nmodel.fit(x_train, y_train)\npredictions = model.predict(validation_data_for_predict)\n\noutput = pd.DataFrame({ \"PassengerId\" : validation_data[\"PassengerId\"], \"Survived\": predictions })\noutput.to_csv(\"/kaggle/working/gender_submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:07:35.502830Z","iopub.execute_input":"2025-01-24T08:07:35.503254Z","iopub.status.idle":"2025-01-24T08:07:35.728529Z","shell.execute_reply.started":"2025-01-24T08:07:35.503215Z","shell.execute_reply":"2025-01-24T08:07:35.727202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Further improvements\n\nAfter implementation there would be more feature extractions such as\n\n1. Age consideration by parsing `Name`: We can guess ages from titles in `Name` such as \n  - 'Countess', 'Lady', 'Sir'\n  - 'Mlle', 'Ms', 'Miss'\n  - 'Mme', 'Mrs'\n  - 'Mr'\n  - 'Master'\n2. Person with non-null `Cabin` has tendency of high survival rate\n\nand applying these features may improve model performance?\n\nAlso it would be better to reorganise the ToC as:\n\n```\nH1. Introduction\nH1. Data Preprocessing - Feature Engineering\n  H2. Data exploration\n    H3. Finding missing values out\n    H3. Planning to fill missing values\n  H2. Data conversion\n    H3. Feature aggregation\n    H3. One-hot encoding\n    H3. Binning\nH1. Training\n```\n\nIt would be better to add hyperparameter tuning process into this material","metadata":{}}]}